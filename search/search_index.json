{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home / Introduction A Model-Based Reinforcement Learning Extension for Stable Baselines. Overview This library provides separate components/ building blocks for four distinct tasks, namely: Sampling Data from a Gym Environement . This step may be not required, if you already have data you want to use to train a dynamic model with. Train a dynamic Model . Either use sampled data from previous step, or your own data to train a dynamic model. Wrap dynamic model in Gym Environment . The framework can wrap the trained model in a gym environemtn with proper action and oberservation space. You only need to define a few details about your input data Train Policy with Stable Baselines . Use any stable baselines algorithm to to train a poliy against the wrapped gym environment. You only have to specify some kind of reward function. Depending on your use-case, you may only use some specific parts of the library. Example Scripts The following list describes what each of the example scripts does. You can find these example scripts in the example_usage folder in the root of the source code of this library ( LINK ). full_application.py : This script showcases the workflow of this library: It demonstrates sampling a gym environment, creating a dynamic model from the sampled data, training a policy with stable baselines against a wrapped gym env using the dynamic model, and finally running the policy against the original gym environment. give_data_and_train.py : Demonstration of how to create and train the dynamic model with given input data. model_wrapping.py : Demonstration of how to wrap a dynamic model as gym environment. multiple_tests.py : Test-Creations of dynamic models for different sampled gym environments for different learning rates and batch sizes . sample_config.yaml : This is the example/base configuration file for the library. sample_env.py : Demonstration of how to sample a gym environment. sample_env_and_train.py : Simple script that samples and trains a respective gym environment for multiple gym environments. sb_policy_creation_example.py : Example how to create a stable baselines policy. stable_baselines_against_wrapped_env.py : Example how to train a stable baselines policy against a wrapped environment (i.e., one, that uses a dynamic model). stable_baselines_test.py : Example how to train a stable baselines policy against the \"original\" gym environments. For some of these scripts to work you must change some settings, e.g. paths to models (which have to be created by you before, too), etc.","title":"Home / Introduction"},{"location":"#home-introduction","text":"A Model-Based Reinforcement Learning Extension for Stable Baselines.","title":"Home / Introduction"},{"location":"#overview","text":"This library provides separate components/ building blocks for four distinct tasks, namely: Sampling Data from a Gym Environement . This step may be not required, if you already have data you want to use to train a dynamic model with. Train a dynamic Model . Either use sampled data from previous step, or your own data to train a dynamic model. Wrap dynamic model in Gym Environment . The framework can wrap the trained model in a gym environemtn with proper action and oberservation space. You only need to define a few details about your input data Train Policy with Stable Baselines . Use any stable baselines algorithm to to train a poliy against the wrapped gym environment. You only have to specify some kind of reward function. Depending on your use-case, you may only use some specific parts of the library.","title":"Overview"},{"location":"#example-scripts","text":"The following list describes what each of the example scripts does. You can find these example scripts in the example_usage folder in the root of the source code of this library ( LINK ). full_application.py : This script showcases the workflow of this library: It demonstrates sampling a gym environment, creating a dynamic model from the sampled data, training a policy with stable baselines against a wrapped gym env using the dynamic model, and finally running the policy against the original gym environment. give_data_and_train.py : Demonstration of how to create and train the dynamic model with given input data. model_wrapping.py : Demonstration of how to wrap a dynamic model as gym environment. multiple_tests.py : Test-Creations of dynamic models for different sampled gym environments for different learning rates and batch sizes . sample_config.yaml : This is the example/base configuration file for the library. sample_env.py : Demonstration of how to sample a gym environment. sample_env_and_train.py : Simple script that samples and trains a respective gym environment for multiple gym environments. sb_policy_creation_example.py : Example how to create a stable baselines policy. stable_baselines_against_wrapped_env.py : Example how to train a stable baselines policy against a wrapped environment (i.e., one, that uses a dynamic model). stable_baselines_test.py : Example how to train a stable baselines policy against the \"original\" gym environments. For some of these scripts to work you must change some settings, e.g. paths to models (which have to be created by you before, too), etc.","title":"Example Scripts"},{"location":"cli/","text":"Command Line Interface The command line interface (CLI) allows you to perform the crucial operations of this library using your terminal. Three main operations are supported: Sampling a gym environment, training a dynamic model, and training a stable baselines policy against the dynamic model. This docs page only gives you an overview, for more details and help use the built-in help and explanations. They can be shown with the --help flag for any command or subcommand. The base command is named sb-mbrl and expects one of the below shown subcommands. An additional command named sb-mbrl-obtain-config-file can be used to create a sample config file. Global Options Configuration File The sb-mbrl command takes the option --config as the path to the config file to use. If it is not specified, a file named config.yaml is expected in the current working directory. --config can be replaced by -c , too. Usage: sb-mbrl --config /path/to/my/config.yaml COMMAND [ARGS] . Debug Usage: sb-mbrl --debug COMMAND [ARGS] . This enables some additional debug features and verbose outputs. Sampling a Gym Environment Usage: sb-mbrl sample [OPTIONS] OUTPUT_DIRECTORY . This command can be used to sample a gym environment name and store the ouput in the OUTPUT_DIRECTORY . The name of the gym environment is read from the config file, unless a custom one is defined via the --gym-env-name option. Addtionally, amount of episodes and max-steps per episode can be defined by --episodes and max-steps options. Example: sb-mbrl -c ./config/config.yaml --debug sample --gym-env-name CartPole-v1 \\ --episodes 10 \\ --max-steps 200 Creating a dynamic Model Usage: sb-mbrl create-dynamic-model [OPTIONS] OUTPUT_DIRECTORY . This command can be used to train a dynamic model using the settings from the provided config. The input data file is also read from the config, unless a custom one is specified via --input-data . The resulting model files (and debug stuff: plots, tensorboard, etc.) are stored in OUTPUT_DIRECTORY . Training a Stable Baselines Policy Usage: sb-mbrl train-stable-baselines-policy [OPTIONS] MODEL_PATH OUTPUT_DIRECTORY . This command can be used to train a Stable Baselines policy against a wrapped gym environment that used a dynamic model created by this library. It takes the path to the model as the MODEL_PATH argument and stores the stable baselines policy in the OUTPUT_DIRECTORY . If custom step- or reset- handlers should be used, they must be implemented in a python file. For details see explanations below. Algorithm, policy and timesteps are obtained from the config file unless overwritten explicitly via the respective option flags. Custom Step Handler If you want to use a custom step handler, you must implement it in a python file that is located in the working directory where you use the CLI. Example: # wrapper.py from stable_baselines_model_based_rl.wrapper.step_handler import StepRewardDoneHandler import math class CustomStepHandler ( StepRewardDoneHandler ): def get_done ( self , step : int ) -> bool : # Angle and x-position at which to fail the episode theta_threshold_radians = 12 * 2 * math . pi / 360 x_threshold = 2.4 cur_state = self . observation . to_value_list () x = cur_state [ 0 ] theta = cur_state [ 2 ] return bool ( x < - x_threshold or x > x_threshold or theta < - theta_threshold_radians or theta > theta_threshold_radians ) Now, provide the --wrapper-step-handler flag and point it to the file (omit the .py extension): sb-mbrl train-stable-baselines-policy \\ --wrapper-step-handler wrapper \\ /path/to/dynamic_model/model.h5 \\ /path/to/output/the/sb_policy/to Custom Reset Handler This works the same way as for custom step handlers, just use the --wrapper-reset-handler flag. The handler class must be named CustomResetHandler . Testing Stable Baselines Policy Usage : sb-mbrl-eval [OPTIONS] SB_ALGORITHM POLICY_PATH GYM_ENV_NAME . This command allows you the evaluate a given (exported) stable baselines policy (path to exported model given with the POLICY_PATH argument) against the original gym environment. The name of the gym environment must be given as the GYM_ENV_NAME argument, as well as the policy that was used, like \"PPO\", or \"AC2\". An optional option --episodes lets you define how many episodes should be run. After all episodes have been run, a short summary will be printed. It looks like this: Maximum amount of steps was: 169 Minimum amount of steps was: 103 Mean of amount of steps was: 127.32 Std of amount of steps was: 14.338674973650807","title":"Command Line Interface"},{"location":"cli/#command-line-interface","text":"The command line interface (CLI) allows you to perform the crucial operations of this library using your terminal. Three main operations are supported: Sampling a gym environment, training a dynamic model, and training a stable baselines policy against the dynamic model. This docs page only gives you an overview, for more details and help use the built-in help and explanations. They can be shown with the --help flag for any command or subcommand. The base command is named sb-mbrl and expects one of the below shown subcommands. An additional command named sb-mbrl-obtain-config-file can be used to create a sample config file.","title":"Command Line Interface"},{"location":"cli/#global-options","text":"","title":"Global Options"},{"location":"cli/#configuration-file","text":"The sb-mbrl command takes the option --config as the path to the config file to use. If it is not specified, a file named config.yaml is expected in the current working directory. --config can be replaced by -c , too. Usage: sb-mbrl --config /path/to/my/config.yaml COMMAND [ARGS] .","title":"Configuration File"},{"location":"cli/#debug","text":"Usage: sb-mbrl --debug COMMAND [ARGS] . This enables some additional debug features and verbose outputs.","title":"Debug"},{"location":"cli/#sampling-a-gym-environment","text":"Usage: sb-mbrl sample [OPTIONS] OUTPUT_DIRECTORY . This command can be used to sample a gym environment name and store the ouput in the OUTPUT_DIRECTORY . The name of the gym environment is read from the config file, unless a custom one is defined via the --gym-env-name option. Addtionally, amount of episodes and max-steps per episode can be defined by --episodes and max-steps options. Example: sb-mbrl -c ./config/config.yaml --debug sample --gym-env-name CartPole-v1 \\ --episodes 10 \\ --max-steps 200","title":"Sampling a Gym Environment"},{"location":"cli/#creating-a-dynamic-model","text":"Usage: sb-mbrl create-dynamic-model [OPTIONS] OUTPUT_DIRECTORY . This command can be used to train a dynamic model using the settings from the provided config. The input data file is also read from the config, unless a custom one is specified via --input-data . The resulting model files (and debug stuff: plots, tensorboard, etc.) are stored in OUTPUT_DIRECTORY .","title":"Creating a dynamic Model"},{"location":"cli/#training-a-stable-baselines-policy","text":"Usage: sb-mbrl train-stable-baselines-policy [OPTIONS] MODEL_PATH OUTPUT_DIRECTORY . This command can be used to train a Stable Baselines policy against a wrapped gym environment that used a dynamic model created by this library. It takes the path to the model as the MODEL_PATH argument and stores the stable baselines policy in the OUTPUT_DIRECTORY . If custom step- or reset- handlers should be used, they must be implemented in a python file. For details see explanations below. Algorithm, policy and timesteps are obtained from the config file unless overwritten explicitly via the respective option flags.","title":"Training a Stable Baselines Policy"},{"location":"cli/#custom-step-handler","text":"If you want to use a custom step handler, you must implement it in a python file that is located in the working directory where you use the CLI. Example: # wrapper.py from stable_baselines_model_based_rl.wrapper.step_handler import StepRewardDoneHandler import math class CustomStepHandler ( StepRewardDoneHandler ): def get_done ( self , step : int ) -> bool : # Angle and x-position at which to fail the episode theta_threshold_radians = 12 * 2 * math . pi / 360 x_threshold = 2.4 cur_state = self . observation . to_value_list () x = cur_state [ 0 ] theta = cur_state [ 2 ] return bool ( x < - x_threshold or x > x_threshold or theta < - theta_threshold_radians or theta > theta_threshold_radians ) Now, provide the --wrapper-step-handler flag and point it to the file (omit the .py extension): sb-mbrl train-stable-baselines-policy \\ --wrapper-step-handler wrapper \\ /path/to/dynamic_model/model.h5 \\ /path/to/output/the/sb_policy/to","title":"Custom Step Handler"},{"location":"cli/#custom-reset-handler","text":"This works the same way as for custom step handlers, just use the --wrapper-reset-handler flag. The handler class must be named CustomResetHandler .","title":"Custom Reset Handler"},{"location":"cli/#testing-stable-baselines-policy","text":"Usage : sb-mbrl-eval [OPTIONS] SB_ALGORITHM POLICY_PATH GYM_ENV_NAME . This command allows you the evaluate a given (exported) stable baselines policy (path to exported model given with the POLICY_PATH argument) against the original gym environment. The name of the gym environment must be given as the GYM_ENV_NAME argument, as well as the policy that was used, like \"PPO\", or \"AC2\". An optional option --episodes lets you define how many episodes should be run. After all episodes have been run, a short summary will be printed. It looks like this: Maximum amount of steps was: 169 Minimum amount of steps was: 103 Mean of amount of steps was: 127.32 Std of amount of steps was: 14.338674973650807","title":"Testing Stable Baselines Policy"},{"location":"configuration-file/","text":"Configuration File The configuration file is a yaml file containing all required parameters used by different parts of the library. Different sections of the file are used by different components of the library, such as the sampler, or for creating the dynamic model. Thus, not all configuration options may be required to be adjusted by you, depending on what features of the framework/library you use. The sampler will also create a (template) configuration file, based on the gym environment that has been sampled. A template/ example configuration file can be viewed here . Gym Environment Sampling gym_sampling : gym_environment_name : 'CartPole-v1' This setting is only used by the CLI for sampling a gym environment and only in the case, that you do not specify another environemnt to sample. Input Data Configuration Example input_config : action_cols : - A_0 - A_1 action_type : DISCRETE # one of: DISCRETE, MULTI_DISCRETE, BOX discrete_colum_sets : null action_box_bounds : low : null high : null observation_cols : - X_0 - X_1 - X_2 - X_3 observation_bounds : low : null high : null input_file_name : 'train.csv' Configuration of Action Columns List including all CSV-Headers of columns, that represent actions. input_config : action_cols : - A_0 - A_1 action_type : DISCRETE discrete_column_sets : null action_box_bounds : null Type of Action Either DISCRETE, MULTI_DISCRETE, or BOX. For DISCRETE, all action columns represent of of the possible discrete actions. For MULTI_DISCRETE and BOX, the respective configuration keys ( discrete_colum_sets , action_box_bounds ) must be additionally defined. See explanations next. Multi Discrete Action Note : Multi-Discrete Actions are not yet supported! It must be specified, which columns belong to which discrete action: input_config : action_cols : - A_0_0 - A_0_1 - A_1_0 - A_1_1 - A_1_2 action_type : MULTI_DISCRETE discrete_column_sets : - [ A_0_0 , A_0_1 ] - [ A_1_0 , A_1_1 , A_1_2 ] action_box_bounds : null This configuration would represent a multi discrete action consisting of two discrete actions. The first of which would have two discrete actions (A_0_0, A_0_1), the second would have three discrete actions (A_1_0, A_1_1, A_1_2). Box Actions The bounds of the input box must be specified by two lists: one for minimum values, one for maximum values. input_config : action_cols : - A_1 - A_2 - A_3 action_type : BOX discrete_column_sets : null action_box_bounds : low : [ -1 , 3 , 0 ] high : [ 0 , 5 , 3 ] This configuration represents three input actions, the first of which is between -1 and 3, the second between 3 and 5 and the third between 0 and 3. Dynamic Model Creation / Training Training The dynamic_model.training section configures how the training of the dynamic model is performed: dynamic_model : training : train_test_ration : 0.7 # 0.7: 70% of the data is used for training, 30% for testing lag : 4 validation_steps : 100 validation_freq : 1 max_epochs : 50 steps_per_epoch : 1000 learning_rate : 0.05 optimizer : adam batch_size : 64 patience : 15 # Epochs in which no learning success is achieved, after which the training is discontinued For more details the corresponding Keras Documentation might be helpful, too. Keras Model Configuration Internally, for the creation of the tensorflow model the tf.keras.models.model_from_yaml method is used. Thus, you can configure your network architecture easily by providing a proper configuration. (Also see the Keras Docs .) dynamic_model : keras_model : class_name : Sequential config : name : 'my_net' layers : - class_name : LSTM config : units : 50 Validation Currently the validation only contains noise configurations that are used later on in the prepare_data function. Those configurations determine how the training data is modified to simulate noise. The percentage variable defines how much of the data is changed and ranges between 0 and 1. The gaussian normal distribution requires a standard deviation (std) and mean value. If calc_mean is set to true the mean value of a column is calculated and used. If set to False however, the mean value of the normal distribution is each column value. dynamic_model : validation : noise : calc_mean : false std : 0.1 percentage : 0.5 Utility Flags The artificial noise decides whether an artificial noise is added to the data. dynamic_model : utility_flags : log_training : True save : True evaluate_model : True plot_results : True export_model : True artificial_noise : False Model Wrapping Configuration for the Model Wrapping is required for three parts of the wrapping: Reward-, Done-, and Reset-Handling. Example model_wrapping : reward : type : CONSTANT # one of CONSTANT, EVAL, or HANDLER value : 1.0 done : type : CONSTANT # one of CONSTANT, EVAL, or HANDLER value : 200 reset : type : RANDOM # one of RANDOM, STATIC, EPISODE_START, or HANDLER value : buffer : null current_state : null data_file : null Reward Configuration Three different types of Reward Configurations exist: Constant, Eval, and Handler Reward-Handling. Constant Reward-Handling Setting the type to CONSTANT requires to additionally set a value to a constant float value. This is the constant reward given per step. model_wrapping : reward : type : CONSTANT value : 4.5 Eval Reward-Handling Setting the type to EVAL requires to additionally set a value to an eval expression that is used to calculate the reward. The eval expression must be valid Python code. The Python eval-function is used for the calculation, respectively the given value-string is applied on the eval-function. Within the context of the eval-function (i.e., within the given string/expression) the current (new) state (observation) and the applied action (that lead to the new state) are available via variables. The names of the variables are the ones given in the input- configuration of the configuration file ( action_cols , observation_cols ). model_wrapping : reward : type : EVAL value : \"(A_0 + A_1) / (X_0**2 * X_1)\" Handler Reward-Handling If the type is set to HANDLER , the user is required to implement a custom handler for reward-calulation, which allows a more complex calculation. For details, check out the docs section regarding the step handler . model_wrapping : reward : type : HANDLER value : null Done Configuration Three different types of Done Configurations exist: Constant, Eval, and Handler Done-Handling. Constant Done-Handling Setting the type to CONSTANT requires to additionally set a value to a constant int value. This is the constant amount of steps after which each episode ends. model_wrapping : done : type : CONSTANT value : 450 Eval Done-Handling Setting the type to EVAL requires to additionally set a value to an eval expression that is used to calculate the done-state. It behaves similar to the eval-reward calculation model_wrapping : done : type : EVAL value : \"(X_0 + X_1)**2 > 100\" Handler Done-Handling If the type is set to HANDLER , the user is required to implement a custom handler for done-calulation, which allows a more complex calculation. For details, check out the docs section regarding the step handler . model_wrapping : done : type : HANDLER value : null Reset Configuration Four different types of Reset-Handling exist: Random-, Static-, Episode-Start-, and Handler-Handling. Random Reset-Handling If the type is set to RANDOM the built-in (randomn) sampling of gym environments is used to create the reset state/observation. If using a window_size > 1 this will most likely produce observations and actions that would not occur together in the \"regular environment\". Thus, this handling is not accurate for larger window_sizes. model_wrapping : reset : type : RANDOM value : null data_file : null Static Reset-Handling If the type is set to STATIC , the same reset observation and state are used for every reset. Depending on the window_size, a list of states must be given to fill the buffer initally. Also, the current state/observation must be given. For a window_size of 3, for example, 2 values for the buffer plus one current observation must be given: model_wrapping : reset : type : STATIC value : buffer : - [ 1 , 0 , 1.0 , 2.0 , 3.0 ] - [ 0 , 1 , 2.0 , 3.0 , 2.5 ] current_state : [ 0.5 , 2.0 , 1.0 ] data_file : null Note, that the buffer includes action and observation values (in the given order), while the current_state only consists of an observation. Episode-Start Reset-Handling If the type is set to EPISODE_START , the reset-state and observation is read from a given data-file (format must equal the file used for training the modes; it can also be the same file, of course). From the given file, one random episode is chosen and used for the reset. model_wrapping : reset : type : EPISODE_START value : null data_file : \"/path/to/the/data/file.csv\" Handler Reset-Handling If the type is set to HANDLER , the user is required to implement a custom handler for the calculation of reset states. This allows a more complex calculation. For details, check out the docs section regarding the reset handler . model_wrapping : reset : type : HANDLER value : null data_file : null Policy Configuration The sb_policy section defines the parameters for the policy training with stable baselines against the (wrapped) gym environment. You have three configuration options here: reinforcement_learning_algorithm : The Algorithm to use for the model (e.g., PPO). policy : The policy to use (e.g., MlpPolicy). timesteps : Timesteps for the training/learning. For more details, check out the API docs and the docs of stable baselines.","title":"Configuration File"},{"location":"configuration-file/#configuration-file","text":"The configuration file is a yaml file containing all required parameters used by different parts of the library. Different sections of the file are used by different components of the library, such as the sampler, or for creating the dynamic model. Thus, not all configuration options may be required to be adjusted by you, depending on what features of the framework/library you use. The sampler will also create a (template) configuration file, based on the gym environment that has been sampled. A template/ example configuration file can be viewed here .","title":"Configuration File"},{"location":"configuration-file/#gym-environment-sampling","text":"gym_sampling : gym_environment_name : 'CartPole-v1' This setting is only used by the CLI for sampling a gym environment and only in the case, that you do not specify another environemnt to sample.","title":"Gym Environment Sampling"},{"location":"configuration-file/#input-data-configuration","text":"","title":"Input Data Configuration"},{"location":"configuration-file/#example","text":"input_config : action_cols : - A_0 - A_1 action_type : DISCRETE # one of: DISCRETE, MULTI_DISCRETE, BOX discrete_colum_sets : null action_box_bounds : low : null high : null observation_cols : - X_0 - X_1 - X_2 - X_3 observation_bounds : low : null high : null input_file_name : 'train.csv'","title":"Example"},{"location":"configuration-file/#configuration-of-action-columns","text":"List including all CSV-Headers of columns, that represent actions. input_config : action_cols : - A_0 - A_1 action_type : DISCRETE discrete_column_sets : null action_box_bounds : null","title":"Configuration of Action Columns"},{"location":"configuration-file/#type-of-action","text":"Either DISCRETE, MULTI_DISCRETE, or BOX. For DISCRETE, all action columns represent of of the possible discrete actions. For MULTI_DISCRETE and BOX, the respective configuration keys ( discrete_colum_sets , action_box_bounds ) must be additionally defined. See explanations next.","title":"Type of Action"},{"location":"configuration-file/#multi-discrete-action","text":"Note : Multi-Discrete Actions are not yet supported! It must be specified, which columns belong to which discrete action: input_config : action_cols : - A_0_0 - A_0_1 - A_1_0 - A_1_1 - A_1_2 action_type : MULTI_DISCRETE discrete_column_sets : - [ A_0_0 , A_0_1 ] - [ A_1_0 , A_1_1 , A_1_2 ] action_box_bounds : null This configuration would represent a multi discrete action consisting of two discrete actions. The first of which would have two discrete actions (A_0_0, A_0_1), the second would have three discrete actions (A_1_0, A_1_1, A_1_2).","title":"Multi Discrete Action"},{"location":"configuration-file/#box-actions","text":"The bounds of the input box must be specified by two lists: one for minimum values, one for maximum values. input_config : action_cols : - A_1 - A_2 - A_3 action_type : BOX discrete_column_sets : null action_box_bounds : low : [ -1 , 3 , 0 ] high : [ 0 , 5 , 3 ] This configuration represents three input actions, the first of which is between -1 and 3, the second between 3 and 5 and the third between 0 and 3.","title":"Box Actions"},{"location":"configuration-file/#dynamic-model-creation-training","text":"","title":"Dynamic Model Creation / Training"},{"location":"configuration-file/#training","text":"The dynamic_model.training section configures how the training of the dynamic model is performed: dynamic_model : training : train_test_ration : 0.7 # 0.7: 70% of the data is used for training, 30% for testing lag : 4 validation_steps : 100 validation_freq : 1 max_epochs : 50 steps_per_epoch : 1000 learning_rate : 0.05 optimizer : adam batch_size : 64 patience : 15 # Epochs in which no learning success is achieved, after which the training is discontinued For more details the corresponding Keras Documentation might be helpful, too.","title":"Training"},{"location":"configuration-file/#keras-model-configuration","text":"Internally, for the creation of the tensorflow model the tf.keras.models.model_from_yaml method is used. Thus, you can configure your network architecture easily by providing a proper configuration. (Also see the Keras Docs .) dynamic_model : keras_model : class_name : Sequential config : name : 'my_net' layers : - class_name : LSTM config : units : 50","title":"Keras Model Configuration"},{"location":"configuration-file/#validation","text":"Currently the validation only contains noise configurations that are used later on in the prepare_data function. Those configurations determine how the training data is modified to simulate noise. The percentage variable defines how much of the data is changed and ranges between 0 and 1. The gaussian normal distribution requires a standard deviation (std) and mean value. If calc_mean is set to true the mean value of a column is calculated and used. If set to False however, the mean value of the normal distribution is each column value. dynamic_model : validation : noise : calc_mean : false std : 0.1 percentage : 0.5","title":"Validation"},{"location":"configuration-file/#utility-flags","text":"The artificial noise decides whether an artificial noise is added to the data. dynamic_model : utility_flags : log_training : True save : True evaluate_model : True plot_results : True export_model : True artificial_noise : False","title":"Utility Flags"},{"location":"configuration-file/#model-wrapping","text":"Configuration for the Model Wrapping is required for three parts of the wrapping: Reward-, Done-, and Reset-Handling.","title":"Model Wrapping"},{"location":"configuration-file/#example_1","text":"model_wrapping : reward : type : CONSTANT # one of CONSTANT, EVAL, or HANDLER value : 1.0 done : type : CONSTANT # one of CONSTANT, EVAL, or HANDLER value : 200 reset : type : RANDOM # one of RANDOM, STATIC, EPISODE_START, or HANDLER value : buffer : null current_state : null data_file : null","title":"Example"},{"location":"configuration-file/#reward-configuration","text":"Three different types of Reward Configurations exist: Constant, Eval, and Handler Reward-Handling.","title":"Reward Configuration"},{"location":"configuration-file/#constant-reward-handling","text":"Setting the type to CONSTANT requires to additionally set a value to a constant float value. This is the constant reward given per step. model_wrapping : reward : type : CONSTANT value : 4.5","title":"Constant Reward-Handling"},{"location":"configuration-file/#eval-reward-handling","text":"Setting the type to EVAL requires to additionally set a value to an eval expression that is used to calculate the reward. The eval expression must be valid Python code. The Python eval-function is used for the calculation, respectively the given value-string is applied on the eval-function. Within the context of the eval-function (i.e., within the given string/expression) the current (new) state (observation) and the applied action (that lead to the new state) are available via variables. The names of the variables are the ones given in the input- configuration of the configuration file ( action_cols , observation_cols ). model_wrapping : reward : type : EVAL value : \"(A_0 + A_1) / (X_0**2 * X_1)\"","title":"Eval Reward-Handling"},{"location":"configuration-file/#handler-reward-handling","text":"If the type is set to HANDLER , the user is required to implement a custom handler for reward-calulation, which allows a more complex calculation. For details, check out the docs section regarding the step handler . model_wrapping : reward : type : HANDLER value : null","title":"Handler Reward-Handling"},{"location":"configuration-file/#done-configuration","text":"Three different types of Done Configurations exist: Constant, Eval, and Handler Done-Handling.","title":"Done Configuration"},{"location":"configuration-file/#constant-done-handling","text":"Setting the type to CONSTANT requires to additionally set a value to a constant int value. This is the constant amount of steps after which each episode ends. model_wrapping : done : type : CONSTANT value : 450","title":"Constant Done-Handling"},{"location":"configuration-file/#eval-done-handling","text":"Setting the type to EVAL requires to additionally set a value to an eval expression that is used to calculate the done-state. It behaves similar to the eval-reward calculation model_wrapping : done : type : EVAL value : \"(X_0 + X_1)**2 > 100\"","title":"Eval Done-Handling"},{"location":"configuration-file/#handler-done-handling","text":"If the type is set to HANDLER , the user is required to implement a custom handler for done-calulation, which allows a more complex calculation. For details, check out the docs section regarding the step handler . model_wrapping : done : type : HANDLER value : null","title":"Handler Done-Handling"},{"location":"configuration-file/#reset-configuration","text":"Four different types of Reset-Handling exist: Random-, Static-, Episode-Start-, and Handler-Handling.","title":"Reset Configuration"},{"location":"configuration-file/#random-reset-handling","text":"If the type is set to RANDOM the built-in (randomn) sampling of gym environments is used to create the reset state/observation. If using a window_size > 1 this will most likely produce observations and actions that would not occur together in the \"regular environment\". Thus, this handling is not accurate for larger window_sizes. model_wrapping : reset : type : RANDOM value : null data_file : null","title":"Random Reset-Handling"},{"location":"configuration-file/#static-reset-handling","text":"If the type is set to STATIC , the same reset observation and state are used for every reset. Depending on the window_size, a list of states must be given to fill the buffer initally. Also, the current state/observation must be given. For a window_size of 3, for example, 2 values for the buffer plus one current observation must be given: model_wrapping : reset : type : STATIC value : buffer : - [ 1 , 0 , 1.0 , 2.0 , 3.0 ] - [ 0 , 1 , 2.0 , 3.0 , 2.5 ] current_state : [ 0.5 , 2.0 , 1.0 ] data_file : null Note, that the buffer includes action and observation values (in the given order), while the current_state only consists of an observation.","title":"Static Reset-Handling"},{"location":"configuration-file/#episode-start-reset-handling","text":"If the type is set to EPISODE_START , the reset-state and observation is read from a given data-file (format must equal the file used for training the modes; it can also be the same file, of course). From the given file, one random episode is chosen and used for the reset. model_wrapping : reset : type : EPISODE_START value : null data_file : \"/path/to/the/data/file.csv\"","title":"Episode-Start Reset-Handling"},{"location":"configuration-file/#handler-reset-handling","text":"If the type is set to HANDLER , the user is required to implement a custom handler for the calculation of reset states. This allows a more complex calculation. For details, check out the docs section regarding the reset handler . model_wrapping : reset : type : HANDLER value : null data_file : null","title":"Handler Reset-Handling"},{"location":"configuration-file/#policy-configuration","text":"The sb_policy section defines the parameters for the policy training with stable baselines against the (wrapped) gym environment. You have three configuration options here: reinforcement_learning_algorithm : The Algorithm to use for the model (e.g., PPO). policy : The policy to use (e.g., MlpPolicy). timesteps : Timesteps for the training/learning. For more details, check out the API docs and the docs of stable baselines.","title":"Policy Configuration"},{"location":"dynamic-model-training/","text":"Dynamic Model Training/Creation Process The dynamic_model_trainer.training module is used to create and train a dynamic model of a gym environment. The module contains a single method build_and_train_dynamic_model that triggers the following process: 1. Prepare data Data processing, executed in prepare_data.prepare_data includes the following steps: Importing the csv file as pandas.DataFrame Adding noise Dividing the data set into training, validation and test data Grouping of samples into fixed-length time-dependent series required for recurrent neural network training Calculating standard deviation and mean required for normalization 2. Build dynamic model model_builder.build_dynamic_model reads the tensorflow model from the configuration file. Normalization layers are added to the front and back of the model, as well as a penultimate output layer in the correct dimension for the given problem. 3. Train the dynamic model 4. Verification In a final step, the training process is reviewed. For this purpose, methods are available in the verifier that check the model against the test data and display it graphically. 5. Save the dynamic model Example Usage config = Configuration ( 'path_to_config' ) training . build_and_train_dynamic_model ( 'path_to_csv' , config , 'path_to_output' )","title":"Dynamic Model Training/Creation"},{"location":"dynamic-model-training/#dynamic-model-trainingcreation","text":"","title":"Dynamic Model Training/Creation"},{"location":"dynamic-model-training/#process","text":"The dynamic_model_trainer.training module is used to create and train a dynamic model of a gym environment. The module contains a single method build_and_train_dynamic_model that triggers the following process: 1. Prepare data Data processing, executed in prepare_data.prepare_data includes the following steps: Importing the csv file as pandas.DataFrame Adding noise Dividing the data set into training, validation and test data Grouping of samples into fixed-length time-dependent series required for recurrent neural network training Calculating standard deviation and mean required for normalization 2. Build dynamic model model_builder.build_dynamic_model reads the tensorflow model from the configuration file. Normalization layers are added to the front and back of the model, as well as a penultimate output layer in the correct dimension for the given problem. 3. Train the dynamic model 4. Verification In a final step, the training process is reviewed. For this purpose, methods are available in the verifier that check the model against the test data and display it graphically. 5. Save the dynamic model","title":"Process"},{"location":"dynamic-model-training/#example-usage","text":"config = Configuration ( 'path_to_config' ) training . build_and_train_dynamic_model ( 'path_to_csv' , config , 'path_to_output' )","title":"Example Usage"},{"location":"environment-wrapping/","text":"Create wrapped Gym Environment The dynamic model that has been created using this library can be wrapped in an gym environment. This allows to run any algorithm against it to learn a policy, e.g. with stable baselines. The WrappedModelEnv requires the model that is used to predict new states after applying certain actions. Basic Configuration / Setup The most basic wrapping may look like this: cfg = Configuration ( 'path/to/config.yaml' ) env = WrappedModelEnv ( 'path/to/exported/model.h5' , config = cfg ) This would use all default settings. However, especially for done and reward calculation you should provide some more sophisticated settings or handlers. Reward- and Done-Calculations are handled by the so called Step Handler . Another handler, called Reset Handler , takes care about resetting the environment. Both are described in more detail below. Step Handler The default implementation of the Step Handler offers a few possibilities to calculate the reward and the done-state for a given (current) observation. For more details, see the respective section in the documentation of the configuration file . For more sophisticated use cases, you can implement your own step handler (derive from the base version!) and provide methods for calculating reward and the done state. For this, you have to overwrite the get_reward and get_done method. It is ok to only overwrite one method and use the default implementation for the other one. Both methods receive the current step number as argument. Additionally, the step handler has access to the following for object/class variables: self.observation : The current observation of the environment. self.action : The last action performed, i.e., the action that lead to the current observation. self.observation_history : A list (history) with all former observations. self.action_history : A list (history) with all former applied actions. Here is an example of how to implement a step handler (only get_done()) for the \"CartPole-v1\" environment. It \"clones\" the framework behaviour for getting the done state: class CartPoleStepHandler ( StepRewardDoneHandler ): def get_done ( self , step : int ) -> bool : # Angle and x-position at which to fail the episode theta_threshold_radians = 12 * 2 * math . pi / 360 x_threshold = 2.4 cur_state = self . observation . to_value_list () x = cur_state [ 0 ] theta = cur_state [ 2 ] return bool ( x < - x_threshold or x > x_threshold or theta < - theta_threshold_radians or theta > theta_threshold_radians ) model_file_path = '...' config = Configuration ( '...' ) step_handler = CartPoleStepHandler ( config ) WrappedModelEnv ( model_file_path , config , step_handler ) Also have a look at the scripts in the example_usage folder. Some example step handlers for gym environments are also available in the stable_baselines_model_based_rl.wrapper.gym_step_handlers module. Reset Handler The default reset handler provides a few basic ways to reset an environment. Take a look at the respective section in the documentation of the configuration file . For most use cases, the EPISODE_START type might be sufficient. For even more sophisticated use cases, you can implement your own reset handler and provide it to the WrappedModelEnv constructor (via the reset_handler argument). Therefore, you must overwrite the generate_reset_observation method, which receives the action_space , observation_space and window_size as inputs. It must return a tuple with a buffer as first item and the current state as second item. The buffer must contain window_size - 1 items (i.e. observations), together with the current state it is used as first input into the dynamic model / neural network for the prediction of the next state. Checkout the API documentation and the default implementations in the ResetHandler for more details. Using the Gym Environment After you have initialized the wrapped gym environment, you can use it like any other gym environment, too. E.g., you can train a stable baselines policy against it. For examples, checkout the example_usage directory. The following is an example of wrapping a dynamic model that trained the CartPole-v1 environemnt. The custom step handler listed above is used. # Initialization model_file_path = './path/to/model.h5' config = Configuration ( './path/to/config.yaml' ) step_handler = CartPoleStepHandler ( config ) env = WrappedModelEnv ( model_file_path , config , step_handler = step_handler ) env . reset () # Usage observation , reward , done , info = env . step ( 1 )","title":"Create wrapped Gym Environment"},{"location":"environment-wrapping/#create-wrapped-gym-environment","text":"The dynamic model that has been created using this library can be wrapped in an gym environment. This allows to run any algorithm against it to learn a policy, e.g. with stable baselines. The WrappedModelEnv requires the model that is used to predict new states after applying certain actions.","title":"Create wrapped Gym Environment"},{"location":"environment-wrapping/#basic-configuration-setup","text":"The most basic wrapping may look like this: cfg = Configuration ( 'path/to/config.yaml' ) env = WrappedModelEnv ( 'path/to/exported/model.h5' , config = cfg ) This would use all default settings. However, especially for done and reward calculation you should provide some more sophisticated settings or handlers. Reward- and Done-Calculations are handled by the so called Step Handler . Another handler, called Reset Handler , takes care about resetting the environment. Both are described in more detail below.","title":"Basic Configuration / Setup"},{"location":"environment-wrapping/#step-handler","text":"The default implementation of the Step Handler offers a few possibilities to calculate the reward and the done-state for a given (current) observation. For more details, see the respective section in the documentation of the configuration file . For more sophisticated use cases, you can implement your own step handler (derive from the base version!) and provide methods for calculating reward and the done state. For this, you have to overwrite the get_reward and get_done method. It is ok to only overwrite one method and use the default implementation for the other one. Both methods receive the current step number as argument. Additionally, the step handler has access to the following for object/class variables: self.observation : The current observation of the environment. self.action : The last action performed, i.e., the action that lead to the current observation. self.observation_history : A list (history) with all former observations. self.action_history : A list (history) with all former applied actions. Here is an example of how to implement a step handler (only get_done()) for the \"CartPole-v1\" environment. It \"clones\" the framework behaviour for getting the done state: class CartPoleStepHandler ( StepRewardDoneHandler ): def get_done ( self , step : int ) -> bool : # Angle and x-position at which to fail the episode theta_threshold_radians = 12 * 2 * math . pi / 360 x_threshold = 2.4 cur_state = self . observation . to_value_list () x = cur_state [ 0 ] theta = cur_state [ 2 ] return bool ( x < - x_threshold or x > x_threshold or theta < - theta_threshold_radians or theta > theta_threshold_radians ) model_file_path = '...' config = Configuration ( '...' ) step_handler = CartPoleStepHandler ( config ) WrappedModelEnv ( model_file_path , config , step_handler ) Also have a look at the scripts in the example_usage folder. Some example step handlers for gym environments are also available in the stable_baselines_model_based_rl.wrapper.gym_step_handlers module.","title":"Step Handler"},{"location":"environment-wrapping/#reset-handler","text":"The default reset handler provides a few basic ways to reset an environment. Take a look at the respective section in the documentation of the configuration file . For most use cases, the EPISODE_START type might be sufficient. For even more sophisticated use cases, you can implement your own reset handler and provide it to the WrappedModelEnv constructor (via the reset_handler argument). Therefore, you must overwrite the generate_reset_observation method, which receives the action_space , observation_space and window_size as inputs. It must return a tuple with a buffer as first item and the current state as second item. The buffer must contain window_size - 1 items (i.e. observations), together with the current state it is used as first input into the dynamic model / neural network for the prediction of the next state. Checkout the API documentation and the default implementations in the ResetHandler for more details.","title":"Reset Handler"},{"location":"environment-wrapping/#using-the-gym-environment","text":"After you have initialized the wrapped gym environment, you can use it like any other gym environment, too. E.g., you can train a stable baselines policy against it. For examples, checkout the example_usage directory. The following is an example of wrapping a dynamic model that trained the CartPole-v1 environemnt. The custom step handler listed above is used. # Initialization model_file_path = './path/to/model.h5' config = Configuration ( './path/to/config.yaml' ) step_handler = CartPoleStepHandler ( config ) env = WrappedModelEnv ( model_file_path , config , step_handler = step_handler ) env . reset () # Usage observation , reward , done , info = env . step ( 1 )","title":"Using the Gym Environment"},{"location":"installation/","text":"Installation Installation using pip (\"Online\") Usually, you would install the library within a virtual environment using the following command: # Installation pip install git+https://github.com/micheltokic/stable_baselines_model_based_rl.git # CLI Usage / Test sb-mbrl --help Installation via Git Clone Alternatively, you can also clone the repository and install the library afterwards using pip again: # Clone Repository git clone git@github.com:micheltokic/stable_baselines_model_based_rl.git # Install using pip cd stable_baselines_model_based_rl pip install . # CLI Usage / Test sb-mbrl --help","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#installation-using-pip-online","text":"Usually, you would install the library within a virtual environment using the following command: # Installation pip install git+https://github.com/micheltokic/stable_baselines_model_based_rl.git # CLI Usage / Test sb-mbrl --help","title":"Installation using pip (\"Online\")"},{"location":"installation/#installation-via-git-clone","text":"Alternatively, you can also clone the repository and install the library afterwards using pip again: # Clone Repository git clone git@github.com:micheltokic/stable_baselines_model_based_rl.git # Install using pip cd stable_baselines_model_based_rl pip install . # CLI Usage / Test sb-mbrl --help","title":"Installation via Git Clone"},{"location":"policy-training/","text":"Policy Training (with Stable Baselines) You can use the train_stable_baselines_policy function from the sb_training module to train/ learn a stable baselines policy against a gym environment. It works for both, regular gym envs, as well as for the wrapped gym env of this library. It is basically just a simple wrapper/ utility function to create the proper policy/model based on the given configuration. For more details, check out the sb_policy_creation_example.py script from the example_usage directory, the API documentation of the above mentionend function, as well as the documentation of stable baselines itself. To make use of all stable baselines features, you can always manually pass and use the wrapped gym environment.","title":"Policy Training (with Stable Baselines)"},{"location":"policy-training/#policy-training-with-stable-baselines","text":"You can use the train_stable_baselines_policy function from the sb_training module to train/ learn a stable baselines policy against a gym environment. It works for both, regular gym envs, as well as for the wrapped gym env of this library. It is basically just a simple wrapper/ utility function to create the proper policy/model based on the given configuration. For more details, check out the sb_policy_creation_example.py script from the example_usage directory, the API documentation of the above mentionend function, as well as the documentation of stable baselines itself. To make use of all stable baselines features, you can always manually pass and use the wrapped gym environment.","title":"Policy Training (with Stable Baselines)"},{"location":"sampling/","text":"Gym Sampling General The sampler.gym_sampler module provides the sample_gym_environment method, that can be used to sample a given gym environement, store the data in a csv file and create a base configuration to be used with this framework. For more details about how to use it, checkout the example_usage directory and the API documentation. Example gym_sampler . sample_gym_environment ( gym_environment_name = 'CartPole-v1' , episode_count = 10 , max_steps = 100 , output_path = './sample_output' , debug = True )","title":"Gym Sampling"},{"location":"sampling/#gym-sampling","text":"","title":"Gym Sampling"},{"location":"sampling/#general","text":"The sampler.gym_sampler module provides the sample_gym_environment method, that can be used to sample a given gym environement, store the data in a csv file and create a base configuration to be used with this framework. For more details about how to use it, checkout the example_usage directory and the API documentation.","title":"General"},{"location":"sampling/#example","text":"gym_sampler . sample_gym_environment ( gym_environment_name = 'CartPole-v1' , episode_count = 10 , max_steps = 100 , output_path = './sample_output' , debug = True )","title":"Example"},{"location":"api/","text":"API Overview Modules cli cli.context cli.sb_mbrl cli.sb_mbrl_eval cli.sb_mbrl_obtain_config_file dynamic_model_trainer dynamic_model_trainer.model_builder dynamic_model_trainer.prepare_data dynamic_model_trainer.training dynamic_model_trainer.verifier sampler sampler.gym_sampler sb_training sb_training.stable_baselines_policy_trainer utils utils.configuration utils.mean utils.noise utils.spaces utils.spaces.base utils.spaces.box utils.spaces.discrete utils.spaces.factory wrapper wrapper.gym_step_handlers wrapper.gym_step_handlers.acrobot wrapper.gym_step_handlers.cartpole wrapper.gym_step_handlers.continuous_mountain_car wrapper.gym_step_handlers.mountain_car wrapper.gym_step_handlers.pendulum wrapper.reset_handler wrapper.step_handler wrapper.wrapped_model_env Classes context.CliContext : A simple utility class holding the context for the different cli commands. configuration.Configuration : Utility class for loading, accessing and modifying the configuration file for this framework. base.SpaceType : Enumeration of different space types (action and observation). base.SpaceValue : A SpaceValue represents a wrapper for a given gym space (like Box, or Discrete) and concrete box.BoxSpaceValue discrete.DiscreteSpaceValue acrobot.AcrobotStepHandler cartpole.CartPoleStepHandler continuous_mountain_car.ContinuousMountainCarStepHandler mountain_car.MountainCarStepHandler pendulum.PendulumStepHandler reset_handler.ResetHandler : A ResetHandler generates the \"reset-observation\" state for a (wrapped) gym environment. step_handler.StepRewardDoneHandler : The reward handler is responsible for calculating the reward for the current state and the wrapped_model_env.WrappedModelEnv : This class wraps a dynamic model (created with the stable_baselines_model_based_rl library) as Functions model_builder.build_dynamic_model : Creates and compiles a neural network consisting of a 'Long Short Term Memory' layer followed prepare_data.prepare_data : Reads the data of the data frame, training.build_and_train_dynamic_model : Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment verifier.evaluate_model : Evaluate a given model against the longest episode of the given data_frame. verifier.evaluate_model_with_test_data : Evaluate the dynamic model with given test data and return average difference per step. verifier.plot_results : Create a evaluation plot for given data. verifier.save : Save given data to given final_dir_path. gym_sampler.sample_gym_environment : Sample the given gym environment with the given amount of episodes and maximum sb_training.get_sb_class_for_algo : Get the corresponding stable baselines class for the given algorithm name. stable_baselines_policy_trainer.train_stable_baselines_policy : Train a stable baselines policy/ create a respective model. mean.nested_list_mean : Average 2D array to 1D array with averaged columns, regardless of their dimension. noise.add_gaussian_noise : Adds artificial gaussian noise to the data set in the given columns. base.generate_gym_box_space : Create a gym box space and derive parameters from given arguments. Either a dimension factory.space_value_from_gym : Create a SpaceValue instance from the given gym-space \"type\", the respective value and the type gym_step_handlers.get_step_handler_for_gym_env : Return an example step handler for the given gym environemtn name, that uses the pendulum.angle_normalize This file was automatically generated via lazydocs .","title":"Overview"},{"location":"api/#api-overview","text":"","title":"API Overview"},{"location":"api/#modules","text":"cli cli.context cli.sb_mbrl cli.sb_mbrl_eval cli.sb_mbrl_obtain_config_file dynamic_model_trainer dynamic_model_trainer.model_builder dynamic_model_trainer.prepare_data dynamic_model_trainer.training dynamic_model_trainer.verifier sampler sampler.gym_sampler sb_training sb_training.stable_baselines_policy_trainer utils utils.configuration utils.mean utils.noise utils.spaces utils.spaces.base utils.spaces.box utils.spaces.discrete utils.spaces.factory wrapper wrapper.gym_step_handlers wrapper.gym_step_handlers.acrobot wrapper.gym_step_handlers.cartpole wrapper.gym_step_handlers.continuous_mountain_car wrapper.gym_step_handlers.mountain_car wrapper.gym_step_handlers.pendulum wrapper.reset_handler wrapper.step_handler wrapper.wrapped_model_env","title":"Modules"},{"location":"api/#classes","text":"context.CliContext : A simple utility class holding the context for the different cli commands. configuration.Configuration : Utility class for loading, accessing and modifying the configuration file for this framework. base.SpaceType : Enumeration of different space types (action and observation). base.SpaceValue : A SpaceValue represents a wrapper for a given gym space (like Box, or Discrete) and concrete box.BoxSpaceValue discrete.DiscreteSpaceValue acrobot.AcrobotStepHandler cartpole.CartPoleStepHandler continuous_mountain_car.ContinuousMountainCarStepHandler mountain_car.MountainCarStepHandler pendulum.PendulumStepHandler reset_handler.ResetHandler : A ResetHandler generates the \"reset-observation\" state for a (wrapped) gym environment. step_handler.StepRewardDoneHandler : The reward handler is responsible for calculating the reward for the current state and the wrapped_model_env.WrappedModelEnv : This class wraps a dynamic model (created with the stable_baselines_model_based_rl library) as","title":"Classes"},{"location":"api/#functions","text":"model_builder.build_dynamic_model : Creates and compiles a neural network consisting of a 'Long Short Term Memory' layer followed prepare_data.prepare_data : Reads the data of the data frame, training.build_and_train_dynamic_model : Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment verifier.evaluate_model : Evaluate a given model against the longest episode of the given data_frame. verifier.evaluate_model_with_test_data : Evaluate the dynamic model with given test data and return average difference per step. verifier.plot_results : Create a evaluation plot for given data. verifier.save : Save given data to given final_dir_path. gym_sampler.sample_gym_environment : Sample the given gym environment with the given amount of episodes and maximum sb_training.get_sb_class_for_algo : Get the corresponding stable baselines class for the given algorithm name. stable_baselines_policy_trainer.train_stable_baselines_policy : Train a stable baselines policy/ create a respective model. mean.nested_list_mean : Average 2D array to 1D array with averaged columns, regardless of their dimension. noise.add_gaussian_noise : Adds artificial gaussian noise to the data set in the given columns. base.generate_gym_box_space : Create a gym box space and derive parameters from given arguments. Either a dimension factory.space_value_from_gym : Create a SpaceValue instance from the given gym-space \"type\", the respective value and the type gym_step_handlers.get_step_handler_for_gym_env : Return an example step handler for the given gym environemtn name, that uses the pendulum.angle_normalize This file was automatically generated via lazydocs .","title":"Functions"},{"location":"api/cli.context/","text":"module cli.context class CliContext A simple utility class holding the context for the different cli commands. method __init__ __init__ ( config : Configuration , debug : bool ) \u2192 None This file was automatically generated via lazydocs .","title":"Cli.context"},{"location":"api/cli.context/#module-clicontext","text":"","title":"module cli.context"},{"location":"api/cli.context/#class-clicontext","text":"A simple utility class holding the context for the different cli commands.","title":"class CliContext"},{"location":"api/cli.context/#method-__init__","text":"__init__ ( config : Configuration , debug : bool ) \u2192 None This file was automatically generated via lazydocs .","title":"method __init__"},{"location":"api/cli/","text":"module cli This file was automatically generated via lazydocs .","title":"Cli"},{"location":"api/cli/#module-cli","text":"This file was automatically generated via lazydocs .","title":"module cli"},{"location":"api/cli.sb_mbrl/","text":"module cli.sb_mbrl This file was automatically generated via lazydocs .","title":"Cli.sb mbrl"},{"location":"api/cli.sb_mbrl/#module-clisb_mbrl","text":"This file was automatically generated via lazydocs .","title":"module cli.sb_mbrl"},{"location":"api/cli.sb_mbrl_eval/","text":"module cli.sb_mbrl_eval This file was automatically generated via lazydocs .","title":"Cli.sb mbrl eval"},{"location":"api/cli.sb_mbrl_eval/#module-clisb_mbrl_eval","text":"This file was automatically generated via lazydocs .","title":"module cli.sb_mbrl_eval"},{"location":"api/cli.sb_mbrl_obtain_config_file/","text":"module cli.sb_mbrl_obtain_config_file This file was automatically generated via lazydocs .","title":"Cli.sb mbrl obtain config file"},{"location":"api/cli.sb_mbrl_obtain_config_file/#module-clisb_mbrl_obtain_config_file","text":"This file was automatically generated via lazydocs .","title":"module cli.sb_mbrl_obtain_config_file"},{"location":"api/dynamic_model_trainer.dynamic_model_trainer/","text":"module dynamic_model_trainer.dynamic_model_trainer function sample_environment_and_train_dynamic_model sample_environment_and_train_dynamic_model ( gym_environment_name , episode_count , max_steps ) Sampling data from a given gym environment and building and training a dynamic model for a given csv dataset retrieved from a csv dataset retrieved from a gym environment based on configurations specified in a yaml file. Args: gym_environment_name : Name of the gym environment from which data is sampled episode_count : Number of episodes to be sampled for the dataset max_steps : Maximum number of steps in an episode log : Specifies whether logging is desired evaluate_model : Specifies whether the evaluation of the model is desired plot_results : Specifies whether plotting is desired export_model : Specifies whether the export of the model is desired Returns: lstm_model : The lstm model trained on the given dataset function build_and_train_dynamic_model build_and_train_dynamic_model ( data_file_name , config_file_name ) Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment based on configurations specified in a yaml file Args: data_file_name : Shape of the input config_file_name : Mean of the targets to determine dense layer length log : Specifies whether logging is desired evaluate_model : Specifies whether the evaluation of the model is desired plot_results : Specifies whether plotting is desired export_model : Specifies whether the export of the model is desired Returns: lstm_model : The lstm model trained on the given dataset Todo: * evaluation, plotting configuration into yaml file? This file was automatically generated via lazydocs .","title":"Dynamic model trainer.dynamic model trainer"},{"location":"api/dynamic_model_trainer.dynamic_model_trainer/#module-dynamic_model_trainerdynamic_model_trainer","text":"","title":"module dynamic_model_trainer.dynamic_model_trainer"},{"location":"api/dynamic_model_trainer.dynamic_model_trainer/#function-sample_environment_and_train_dynamic_model","text":"sample_environment_and_train_dynamic_model ( gym_environment_name , episode_count , max_steps ) Sampling data from a given gym environment and building and training a dynamic model for a given csv dataset retrieved from a csv dataset retrieved from a gym environment based on configurations specified in a yaml file. Args: gym_environment_name : Name of the gym environment from which data is sampled episode_count : Number of episodes to be sampled for the dataset max_steps : Maximum number of steps in an episode log : Specifies whether logging is desired evaluate_model : Specifies whether the evaluation of the model is desired plot_results : Specifies whether plotting is desired export_model : Specifies whether the export of the model is desired Returns: lstm_model : The lstm model trained on the given dataset","title":"function sample_environment_and_train_dynamic_model"},{"location":"api/dynamic_model_trainer.dynamic_model_trainer/#function-build_and_train_dynamic_model","text":"build_and_train_dynamic_model ( data_file_name , config_file_name ) Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment based on configurations specified in a yaml file Args: data_file_name : Shape of the input config_file_name : Mean of the targets to determine dense layer length log : Specifies whether logging is desired evaluate_model : Specifies whether the evaluation of the model is desired plot_results : Specifies whether plotting is desired export_model : Specifies whether the export of the model is desired Returns: lstm_model : The lstm model trained on the given dataset Todo: * evaluation, plotting configuration into yaml file? This file was automatically generated via lazydocs .","title":"function build_and_train_dynamic_model"},{"location":"api/dynamic_model_trainer/","text":"module dynamic_model_trainer This file was automatically generated via lazydocs .","title":"Dynamic model trainer"},{"location":"api/dynamic_model_trainer/#module-dynamic_model_trainer","text":"This file was automatically generated via lazydocs .","title":"module dynamic_model_trainer"},{"location":"api/dynamic_model_trainer.model_builder/","text":"module dynamic_model_trainer.model_builder function build_dynamic_model build_dynamic_model ( model_config , input_shape , mean_in , std_in , mean_out , std_out , output_len , optimizer ) Creates and compiles a neural network consisting of a 'Long Short Term Memory' layer followed by a 'Dense' layer. Additionally, two Keras/Tensorflow Lambda layers are used for normalization of data (and \"reverting\" of the normalization). Args: model_config : Dictionary containing the model building configuration. input_shape : Shape of the input (first layer). mean_in : Mean of input data. std_in : Standard deviation of input data. mean_out : Mean of output data. std_out : Standard deviation of output data. output_len : Length of the output vector (\"output shape\"). optimizer : Optimizer used optimizing gradient descent. Returns: dynamic_model : Compiled tensorflow model with architecture given in the model_config dictionary This file was automatically generated via lazydocs .","title":"Dynamic model trainer.model builder"},{"location":"api/dynamic_model_trainer.model_builder/#module-dynamic_model_trainermodel_builder","text":"","title":"module dynamic_model_trainer.model_builder"},{"location":"api/dynamic_model_trainer.model_builder/#function-build_dynamic_model","text":"build_dynamic_model ( model_config , input_shape , mean_in , std_in , mean_out , std_out , output_len , optimizer ) Creates and compiles a neural network consisting of a 'Long Short Term Memory' layer followed by a 'Dense' layer. Additionally, two Keras/Tensorflow Lambda layers are used for normalization of data (and \"reverting\" of the normalization). Args: model_config : Dictionary containing the model building configuration. input_shape : Shape of the input (first layer). mean_in : Mean of input data. std_in : Standard deviation of input data. mean_out : Mean of output data. std_out : Standard deviation of output data. output_len : Length of the output vector (\"output shape\"). optimizer : Optimizer used optimizing gradient descent. Returns: dynamic_model : Compiled tensorflow model with architecture given in the model_config dictionary This file was automatically generated via lazydocs .","title":"function build_dynamic_model"},{"location":"api/dynamic_model_trainer.prepare_data/","text":"module dynamic_model_trainer.prepare_data function prepare_data prepare_data ( df , input_col , target_col , window_size , training_batch_size = 10 , validation_batch_size = 10 , training_pattern_percent = 0.7 , noise_settings = {} ) Reads the data of the data frame, Converts them into a dataset readable by tensorflow and splits trainings and validation data. Additionally, the standard deviation and the average of the data are determined. Args: df : DataFrame with sampled Data from a gym environment. input_col : List with the names of the input columns. target_col : List with the names of the target columns. window_size : Number of past time steps that are taken into account training_batch_size : Bach size used for training validation_batch_size : Bach size used for validation training_pattern_percent : Relationship between training and validation data noise_settings : noise settings Returns: train_data : Training data set val_data : Validation data set input_shape : Shape of the input mean_in : Mean of the inputs std_in : Standard deviation of the inputs mean_out : Mean of the targets std_out : Standard deviation of the targets This file was automatically generated via lazydocs .","title":"Dynamic model trainer.prepare data"},{"location":"api/dynamic_model_trainer.prepare_data/#module-dynamic_model_trainerprepare_data","text":"","title":"module dynamic_model_trainer.prepare_data"},{"location":"api/dynamic_model_trainer.prepare_data/#function-prepare_data","text":"prepare_data ( df , input_col , target_col , window_size , training_batch_size = 10 , validation_batch_size = 10 , training_pattern_percent = 0.7 , noise_settings = {} ) Reads the data of the data frame, Converts them into a dataset readable by tensorflow and splits trainings and validation data. Additionally, the standard deviation and the average of the data are determined. Args: df : DataFrame with sampled Data from a gym environment. input_col : List with the names of the input columns. target_col : List with the names of the target columns. window_size : Number of past time steps that are taken into account training_batch_size : Bach size used for training validation_batch_size : Bach size used for validation training_pattern_percent : Relationship between training and validation data noise_settings : noise settings Returns: train_data : Training data set val_data : Validation data set input_shape : Shape of the input mean_in : Mean of the inputs std_in : Standard deviation of the inputs mean_out : Mean of the targets std_out : Standard deviation of the targets This file was automatically generated via lazydocs .","title":"function prepare_data"},{"location":"api/dynamic_model_trainer.tensorflow_data_generator/","text":"module dynamic_model_trainer.tensorflow_data_generator function prepare_data prepare_data ( df , input_col , target_col , window_size , training_batch_size = 10 , validation_batch_size = 10 , training_pattern_percent = 0.7 , noise_settings = {} ) Reads the data of the data frame, Converts them into a dataset readable by tensorflow and splits trainings and validation data. Additionally, the standard deviation and the average of the data are determined. Args: df : DataFrame with sampled Data from a gym environment. input_col : List with the names of the input columns. target_col : List with the names of the target columns. window_size : Number of past time steps that are taken into account training_batch_size : Bach size used for training validation_batch_size : Bach size used for validation training_pattern_percent : Relationship between training and validation data noise_settings : noise settings Returns: train_data : Training data set val_data : Validation data set input_shape : Shape of the input mean_in : Mean of the inputs std_in : Standard deviation of the inputs mean_out : Mean of the targets std_out : Standard deviation of the targets This file was automatically generated via lazydocs .","title":"Dynamic model trainer.tensorflow data generator"},{"location":"api/dynamic_model_trainer.tensorflow_data_generator/#module-dynamic_model_trainertensorflow_data_generator","text":"","title":"module dynamic_model_trainer.tensorflow_data_generator"},{"location":"api/dynamic_model_trainer.tensorflow_data_generator/#function-prepare_data","text":"prepare_data ( df , input_col , target_col , window_size , training_batch_size = 10 , validation_batch_size = 10 , training_pattern_percent = 0.7 , noise_settings = {} ) Reads the data of the data frame, Converts them into a dataset readable by tensorflow and splits trainings and validation data. Additionally, the standard deviation and the average of the data are determined. Args: df : DataFrame with sampled Data from a gym environment. input_col : List with the names of the input columns. target_col : List with the names of the target columns. window_size : Number of past time steps that are taken into account training_batch_size : Bach size used for training validation_batch_size : Bach size used for validation training_pattern_percent : Relationship between training and validation data noise_settings : noise settings Returns: train_data : Training data set val_data : Validation data set input_shape : Shape of the input mean_in : Mean of the inputs std_in : Standard deviation of the inputs mean_out : Mean of the targets std_out : Standard deviation of the targets This file was automatically generated via lazydocs .","title":"function prepare_data"},{"location":"api/dynamic_model_trainer.training/","text":"module dynamic_model_trainer.training Global Variables ROOT_DIR function build_and_train_dynamic_model build_and_train_dynamic_model ( data_path , config : Configuration , output_path = '/home/runner/work/stable_baselines_model_based_rl/stable_baselines_model_based_rl' , debug : bool = False ) Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment based on configurations specified in a yaml file Args: data_path : Name of the data file config : Configuration Object that contains the given yaml Configuration output_path : Directory path of training output debug : Flag to enable additional debug features, such as renaming the directory of the resulting model based on the used lag value and the resulted loss. Returns: model : The model trained on the given dataset output_path : The path were everything has been stored model_file_path : The path to the exported model file (None, if it shouldn't be exported) This file was automatically generated via lazydocs .","title":"Dynamic model trainer.training"},{"location":"api/dynamic_model_trainer.training/#module-dynamic_model_trainertraining","text":"","title":"module dynamic_model_trainer.training"},{"location":"api/dynamic_model_trainer.training/#global-variables","text":"ROOT_DIR","title":"Global Variables"},{"location":"api/dynamic_model_trainer.training/#function-build_and_train_dynamic_model","text":"build_and_train_dynamic_model ( data_path , config : Configuration , output_path = '/home/runner/work/stable_baselines_model_based_rl/stable_baselines_model_based_rl' , debug : bool = False ) Builds and trains a dynamic model for a given csv dataset retrieved from a gym environment based on configurations specified in a yaml file Args: data_path : Name of the data file config : Configuration Object that contains the given yaml Configuration output_path : Directory path of training output debug : Flag to enable additional debug features, such as renaming the directory of the resulting model based on the used lag value and the resulted loss. Returns: model : The model trained on the given dataset output_path : The path were everything has been stored model_file_path : The path to the exported model file (None, if it shouldn't be exported) This file was automatically generated via lazydocs .","title":"function build_and_train_dynamic_model"},{"location":"api/dynamic_model_trainer.verifier/","text":"module dynamic_model_trainer.verifier function evaluate_model_with_test_data evaluate_model_with_test_data ( model , test_data , input_col_names , action_col_names , target_col_names , lag ) Evaluate the dynamic model with given test data and return average difference per step. The average (absolute) difference per step is calculated over all episodes of the test data. Args: model : Dynamic model to use for evaluating predictions. test_data : Data frame with test data, not (!) grouped by episode yet. input_col_names : Names of the input columns for the model (i.e., action and observations/states). action_col_names : Names of the action columns. target_col_names : Names of the target columns, i.e. state columns (columns predicted by the model) lag : Used window-size / lag by the model. Returns: List of average difference per step between prediction and real test data. function evaluate_model evaluate_model ( model , data_frame , input_col_names , action_col_names , target_col_names , lag ) Evaluate a given model against the longest episode of the given data_frame. Args: model : Model data_frame : Data input_col_names : Names of the inputs action_col_names : Names of the action inputs target_col_names : Names of the targets lag : Number of past time steps that are taken into account Returns: dfNet : Predictions of the model. dfEval : DataFrame containing the longest episode of the given data_frame. This is the episode the model was evaluated against. function plot_results plot_results ( target_col_names , action_col_names , dfNet , dfEval , dfDiff , window_size ) Create a evaluation plot for given data. function save save ( final_dir_path , model , fig , config , df , debug ) Save given data to given final_dir_path. In debug mode, the config file and the df (DataFrame = data) are stored, additionally. Args: final_dir_path : Output directory to store the data to. model : Keras Model to export (It'll be only exported, if the corresponding config setting (\"dynamic_model.utility_flags.export_model\") is set to True). fig : Figure to export (optionally, otherwise None). config : Configuration. df : DataFrame containing the data the model was created with debug : Debug Flag: controlls whether df and config are addtionally exported. This file was automatically generated via lazydocs .","title":"Dynamic model trainer.verifier"},{"location":"api/dynamic_model_trainer.verifier/#module-dynamic_model_trainerverifier","text":"","title":"module dynamic_model_trainer.verifier"},{"location":"api/dynamic_model_trainer.verifier/#function-evaluate_model_with_test_data","text":"evaluate_model_with_test_data ( model , test_data , input_col_names , action_col_names , target_col_names , lag ) Evaluate the dynamic model with given test data and return average difference per step. The average (absolute) difference per step is calculated over all episodes of the test data. Args: model : Dynamic model to use for evaluating predictions. test_data : Data frame with test data, not (!) grouped by episode yet. input_col_names : Names of the input columns for the model (i.e., action and observations/states). action_col_names : Names of the action columns. target_col_names : Names of the target columns, i.e. state columns (columns predicted by the model) lag : Used window-size / lag by the model. Returns: List of average difference per step between prediction and real test data.","title":"function evaluate_model_with_test_data"},{"location":"api/dynamic_model_trainer.verifier/#function-evaluate_model","text":"evaluate_model ( model , data_frame , input_col_names , action_col_names , target_col_names , lag ) Evaluate a given model against the longest episode of the given data_frame. Args: model : Model data_frame : Data input_col_names : Names of the inputs action_col_names : Names of the action inputs target_col_names : Names of the targets lag : Number of past time steps that are taken into account Returns: dfNet : Predictions of the model. dfEval : DataFrame containing the longest episode of the given data_frame. This is the episode the model was evaluated against.","title":"function evaluate_model"},{"location":"api/dynamic_model_trainer.verifier/#function-plot_results","text":"plot_results ( target_col_names , action_col_names , dfNet , dfEval , dfDiff , window_size ) Create a evaluation plot for given data.","title":"function plot_results"},{"location":"api/dynamic_model_trainer.verifier/#function-save","text":"save ( final_dir_path , model , fig , config , df , debug ) Save given data to given final_dir_path. In debug mode, the config file and the df (DataFrame = data) are stored, additionally. Args: final_dir_path : Output directory to store the data to. model : Keras Model to export (It'll be only exported, if the corresponding config setting (\"dynamic_model.utility_flags.export_model\") is set to True). fig : Figure to export (optionally, otherwise None). config : Configuration. df : DataFrame containing the data the model was created with debug : Debug Flag: controlls whether df and config are addtionally exported. This file was automatically generated via lazydocs .","title":"function save"},{"location":"api/sampler.gym_sampler/","text":"module sampler.gym_sampler Global Variables ROOT_DIR function sample_gym_environment sample_gym_environment ( gym_environment_name : str , episode_count = 20 , max_steps = 100 , output_path = '/home/runner/work/stable_baselines_model_based_rl/stable_baselines_model_based_rl/sample_output' , debug : bool = False ) Sample the given gym environment with the given amount of episodes and maximum steps per episode. Two files are created: - A CSV file, containing the sampled data. - A YAML file, containing the configuration that results from the sampled gym environment, based on the sample_config.yaml file. Both files are stored within the output_path directory. They will be subfolders of directories containing the gym environment name and the current time. E.g. the follwoing folder structure will be created within output_path: \"CartPole-v1/sample_data/2021-05-01-10-00-30/data.csv\". Args: gym_environment_name : Name of the Gym-Environment to sample. epsiode_count : Amount of episodes to use for the sampling. max_steps : Maximum steps per episode allowed during sampling. output_path : The directory the generated files are stored in. debug : Flag whether to enable debugging features, such as naming the output folder based on the amount of episodes and max steps. Returns: data_file : Path to the created data (csv) file. config : Configuration object created for the sampled environment. This file was automatically generated via lazydocs .","title":"Sampler.gym sampler"},{"location":"api/sampler.gym_sampler/#module-samplergym_sampler","text":"","title":"module sampler.gym_sampler"},{"location":"api/sampler.gym_sampler/#global-variables","text":"ROOT_DIR","title":"Global Variables"},{"location":"api/sampler.gym_sampler/#function-sample_gym_environment","text":"sample_gym_environment ( gym_environment_name : str , episode_count = 20 , max_steps = 100 , output_path = '/home/runner/work/stable_baselines_model_based_rl/stable_baselines_model_based_rl/sample_output' , debug : bool = False ) Sample the given gym environment with the given amount of episodes and maximum steps per episode. Two files are created: - A CSV file, containing the sampled data. - A YAML file, containing the configuration that results from the sampled gym environment, based on the sample_config.yaml file. Both files are stored within the output_path directory. They will be subfolders of directories containing the gym environment name and the current time. E.g. the follwoing folder structure will be created within output_path: \"CartPole-v1/sample_data/2021-05-01-10-00-30/data.csv\". Args: gym_environment_name : Name of the Gym-Environment to sample. epsiode_count : Amount of episodes to use for the sampling. max_steps : Maximum steps per episode allowed during sampling. output_path : The directory the generated files are stored in. debug : Flag whether to enable debugging features, such as naming the output folder based on the amount of episodes and max steps. Returns: data_file : Path to the created data (csv) file. config : Configuration object created for the sampled environment. This file was automatically generated via lazydocs .","title":"function sample_gym_environment"},{"location":"api/sampler/","text":"module sampler This file was automatically generated via lazydocs .","title":"Sampler"},{"location":"api/sampler/#module-sampler","text":"This file was automatically generated via lazydocs .","title":"module sampler"},{"location":"api/sb_training/","text":"module sb_training function get_sb_class_for_algo get_sb_class_for_algo ( algo : str ) \u2192 BaseAlgorithm Get the corresponding stable baselines class for the given algorithm name. This file was automatically generated via lazydocs .","title":"Sb training"},{"location":"api/sb_training/#module-sb_training","text":"","title":"module sb_training"},{"location":"api/sb_training/#function-get_sb_class_for_algo","text":"get_sb_class_for_algo ( algo : str ) \u2192 BaseAlgorithm Get the corresponding stable baselines class for the given algorithm name. This file was automatically generated via lazydocs .","title":"function get_sb_class_for_algo"},{"location":"api/sb_training.stable_baselines_policy_trainer/","text":"module sb_training.stable_baselines_policy_trainer function train_stable_baselines_policy train_stable_baselines_policy ( config : Configuration , env : Env , output_dir : str = None , export : bool = False , debug : bool = False ) Train a stable baselines policy/ create a respective model. This function is a simple utility to learn a stable baselines policy agains any given gym environment. Optionally supports saving/exporting the model into the given output directory. Args: config : (Library) configuration. env : Gym environemnt to learn the policy against. output_dir : Directory where to export the model to (optionally). export : Set to True, if model should be exported. debug : If set to True, verbose output of stable baselines is enabled. Returns: The stable baselines model. This file was automatically generated via lazydocs .","title":"Sb training.stable baselines policy trainer"},{"location":"api/sb_training.stable_baselines_policy_trainer/#module-sb_trainingstable_baselines_policy_trainer","text":"","title":"module sb_training.stable_baselines_policy_trainer"},{"location":"api/sb_training.stable_baselines_policy_trainer/#function-train_stable_baselines_policy","text":"train_stable_baselines_policy ( config : Configuration , env : Env , output_dir : str = None , export : bool = False , debug : bool = False ) Train a stable baselines policy/ create a respective model. This function is a simple utility to learn a stable baselines policy agains any given gym environment. Optionally supports saving/exporting the model into the given output directory. Args: config : (Library) configuration. env : Gym environemnt to learn the policy against. output_dir : Directory where to export the model to (optionally). export : Set to True, if model should be exported. debug : If set to True, verbose output of stable baselines is enabled. Returns: The stable baselines model. This file was automatically generated via lazydocs .","title":"function train_stable_baselines_policy"},{"location":"api/utils.configuration/","text":"module utils.configuration class Configuration Utility class for loading, accessing and modifying the configuration file for this framework. method __init__ __init__ ( file : str ) \u2192 None Create new object and load given configuration file. method get get ( key : str , default = None ) Get a specific value from the configuration by key. Args: key : The key to get the value for. For nested objects/ keys the point (.) is used as separator. E.g. get('foo.bar'). default : Default value to return, if the given key does not have a value assigned Returns: any : the value for the given key. method load_config_from_file load_config_from_file ( file : str ) Load and parse the given configuration (yaml) file into the internal dict. method save_config save_config ( file = None ) Save current (internal dict) state of the configuration to the given file. method set set ( key : str , val ) Set a specific value in the configuration by key. Required nested config objects are created, too. Args: key : The key to set the value for. For nested Objectes the point (.) is used as separator. E.g. set('foo.bar', 12) val : The value to set for the given key This file was automatically generated via lazydocs .","title":"Utils.configuration"},{"location":"api/utils.configuration/#module-utilsconfiguration","text":"","title":"module utils.configuration"},{"location":"api/utils.configuration/#class-configuration","text":"Utility class for loading, accessing and modifying the configuration file for this framework.","title":"class Configuration"},{"location":"api/utils.configuration/#method-__init__","text":"__init__ ( file : str ) \u2192 None Create new object and load given configuration file.","title":"method __init__"},{"location":"api/utils.configuration/#method-get","text":"get ( key : str , default = None ) Get a specific value from the configuration by key. Args: key : The key to get the value for. For nested objects/ keys the point (.) is used as separator. E.g. get('foo.bar'). default : Default value to return, if the given key does not have a value assigned Returns: any : the value for the given key.","title":"method get"},{"location":"api/utils.configuration/#method-load_config_from_file","text":"load_config_from_file ( file : str ) Load and parse the given configuration (yaml) file into the internal dict.","title":"method load_config_from_file"},{"location":"api/utils.configuration/#method-save_config","text":"save_config ( file = None ) Save current (internal dict) state of the configuration to the given file.","title":"method save_config"},{"location":"api/utils.configuration/#method-set","text":"set ( key : str , val ) Set a specific value in the configuration by key. Required nested config objects are created, too. Args: key : The key to set the value for. For nested Objectes the point (.) is used as separator. E.g. set('foo.bar', 12) val : The value to set for the given key This file was automatically generated via lazydocs .","title":"method set"},{"location":"api/utils/","text":"module utils This file was automatically generated via lazydocs .","title":"Utils"},{"location":"api/utils/#module-utils","text":"This file was automatically generated via lazydocs .","title":"module utils"},{"location":"api/utils.mean/","text":"module utils.mean function nested_list_mean nested_list_mean ( two_d_array ) Average 2D array to 1D array with averaged columns, regardless of their dimension. This file was automatically generated via lazydocs .","title":"Utils.mean"},{"location":"api/utils.mean/#module-utilsmean","text":"","title":"module utils.mean"},{"location":"api/utils.mean/#function-nested_list_mean","text":"nested_list_mean ( two_d_array ) Average 2D array to 1D array with averaged columns, regardless of their dimension. This file was automatically generated via lazydocs .","title":"function nested_list_mean"},{"location":"api/utils.noise/","text":"module utils.noise function add_gaussian_noise add_gaussian_noise ( data , columns , calc_mean = True , std = 0 , percentage = 0.5 ) Adds artificial gaussian noise to the data set in the given columns. Args: data : Dataset columns : Columns that are altered calc_mean : Specify if the generated noise should be based off of the mean value of a column or each column value on its own std : Standard deviation percentage : specifies how much of the data is modified by an artificial noise Returns: data : The noisy dataset This file was automatically generated via lazydocs .","title":"Utils.noise"},{"location":"api/utils.noise/#module-utilsnoise","text":"","title":"module utils.noise"},{"location":"api/utils.noise/#function-add_gaussian_noise","text":"add_gaussian_noise ( data , columns , calc_mean = True , std = 0 , percentage = 0.5 ) Adds artificial gaussian noise to the data set in the given columns. Args: data : Dataset columns : Columns that are altered calc_mean : Specify if the generated noise should be based off of the mean value of a column or each column value on its own std : Standard deviation percentage : specifies how much of the data is modified by an artificial noise Returns: data : The noisy dataset This file was automatically generated via lazydocs .","title":"function add_gaussian_noise"},{"location":"api/utils.spaces.base/","text":"module utils.spaces.base function generate_gym_box_space generate_gym_box_space ( dimensions = None , low = None , high = None ) \u2192 Box Create a gym box space and derive parameters from given arguments. Either a dimension must be given or the dimension must be derivable from the length of min/max values. Args: dimensions (int): Amount of dimensions the Box has (shape) low : Single minimum value (for all dimenions) or list of minimum values. In the first case the dimension parameter may not be None. high : Single maximum value (for all dimenions) or list of maximum values. In the first case the dimension parameter may not be None. Returns a Gym Box space with given dimensions and low/high values. class SpaceType Enumeration of different space types (action and observation). class SpaceValue A SpaceValue represents a wrapper for a given gym space (like Box, or Discrete) and concrete values that can either be returnes as the respective \"gym values\" or as values matching the CSV input configuration (i.e., values mapped to respective columns = the internal library representation). The representation is, for example, different for Discrete (Gym: 2, internal library: [0,0,1,0]). This class enables convertions between both different representations. Subclasses implement the logic for specific gym environments. method __init__ __init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None ) \u2192 None method col_amount col_amount () Returns the required amount of columns for this space (value). method from_gym_space from_gym_space ( gym_space : Space , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None ) Create new SpaceValue from gym_space, the respective value and the space type (action or observation). method generate_default_col_names generate_default_col_names ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , amount : int = 1 ) \u2192 List [ str ] Generate a set of x default column names where x = amount. Default names consist of one character (\"A\" for actinos, \"X\" for observations) and a increasing number per column. Args: type : Which type to create column names for (action / observation). amount : Amount of column names to create. Returns a list of default column names. method to_column_dict to_column_dict () \u2192 Dict Map the values of the Space to the respective column names (of the input CSV data file). Returns: dict : A dict with the column names as keys and the respective space values as dict-values. Column names are set during intialization. method to_gym_space to_gym_space () \u2192 Tuple [ Space , Any ] Return the gym space of this value and the corresponding value. method to_value_list to_value_list () This file was automatically generated via lazydocs .","title":"Utils.spaces.base"},{"location":"api/utils.spaces.base/#module-utilsspacesbase","text":"","title":"module utils.spaces.base"},{"location":"api/utils.spaces.base/#function-generate_gym_box_space","text":"generate_gym_box_space ( dimensions = None , low = None , high = None ) \u2192 Box Create a gym box space and derive parameters from given arguments. Either a dimension must be given or the dimension must be derivable from the length of min/max values. Args: dimensions (int): Amount of dimensions the Box has (shape) low : Single minimum value (for all dimenions) or list of minimum values. In the first case the dimension parameter may not be None. high : Single maximum value (for all dimenions) or list of maximum values. In the first case the dimension parameter may not be None. Returns a Gym Box space with given dimensions and low/high values.","title":"function generate_gym_box_space"},{"location":"api/utils.spaces.base/#class-spacetype","text":"Enumeration of different space types (action and observation).","title":"class SpaceType"},{"location":"api/utils.spaces.base/#class-spacevalue","text":"A SpaceValue represents a wrapper for a given gym space (like Box, or Discrete) and concrete values that can either be returnes as the respective \"gym values\" or as values matching the CSV input configuration (i.e., values mapped to respective columns = the internal library representation). The representation is, for example, different for Discrete (Gym: 2, internal library: [0,0,1,0]). This class enables convertions between both different representations. Subclasses implement the logic for specific gym environments.","title":"class SpaceValue"},{"location":"api/utils.spaces.base/#method-__init__","text":"__init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None ) \u2192 None","title":"method __init__"},{"location":"api/utils.spaces.base/#method-col_amount","text":"col_amount () Returns the required amount of columns for this space (value).","title":"method col_amount"},{"location":"api/utils.spaces.base/#method-from_gym_space","text":"from_gym_space ( gym_space : Space , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None ) Create new SpaceValue from gym_space, the respective value and the space type (action or observation).","title":"method from_gym_space"},{"location":"api/utils.spaces.base/#method-generate_default_col_names","text":"generate_default_col_names ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , amount : int = 1 ) \u2192 List [ str ] Generate a set of x default column names where x = amount. Default names consist of one character (\"A\" for actinos, \"X\" for observations) and a increasing number per column. Args: type : Which type to create column names for (action / observation). amount : Amount of column names to create. Returns a list of default column names.","title":"method generate_default_col_names"},{"location":"api/utils.spaces.base/#method-to_column_dict","text":"to_column_dict () \u2192 Dict Map the values of the Space to the respective column names (of the input CSV data file). Returns: dict : A dict with the column names as keys and the respective space values as dict-values. Column names are set during intialization.","title":"method to_column_dict"},{"location":"api/utils.spaces.base/#method-to_gym_space","text":"to_gym_space () \u2192 Tuple [ Space , Any ] Return the gym space of this value and the corresponding value.","title":"method to_gym_space"},{"location":"api/utils.spaces.base/#method-to_value_list","text":"to_value_list () This file was automatically generated via lazydocs .","title":"method to_value_list"},{"location":"api/utils.spaces.box/","text":"module utils.spaces.box class BoxSpaceValue method __init__ __init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None , low = None , high = None ) \u2192 None method from_gym_space from_gym_space ( gym_space : Box , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None ) method to_gym_space to_gym_space () \u2192 Tuple [ Box , Any ] This file was automatically generated via lazydocs .","title":"Utils.spaces.box"},{"location":"api/utils.spaces.box/#module-utilsspacesbox","text":"","title":"module utils.spaces.box"},{"location":"api/utils.spaces.box/#class-boxspacevalue","text":"","title":"class BoxSpaceValue"},{"location":"api/utils.spaces.box/#method-__init__","text":"__init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None , low = None , high = None ) \u2192 None","title":"method __init__"},{"location":"api/utils.spaces.box/#method-from_gym_space","text":"from_gym_space ( gym_space : Box , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None )","title":"method from_gym_space"},{"location":"api/utils.spaces.box/#method-to_gym_space","text":"to_gym_space () \u2192 Tuple [ Box , Any ] This file was automatically generated via lazydocs .","title":"method to_gym_space"},{"location":"api/utils.spaces.discrete/","text":"module utils.spaces.discrete class DiscreteSpaceValue method __init__ __init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None ) \u2192 None method from_gym_space from_gym_space ( gym_space : Discrete , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None ) method to_gym_space to_gym_space () \u2192 Tuple [ Discrete , int ] This file was automatically generated via lazydocs .","title":"Utils.spaces.discrete"},{"location":"api/utils.spaces.discrete/#module-utilsspacesdiscrete","text":"","title":"module utils.spaces.discrete"},{"location":"api/utils.spaces.discrete/#class-discretespacevalue","text":"","title":"class DiscreteSpaceValue"},{"location":"api/utils.spaces.discrete/#method-__init__","text":"__init__ ( type : SpaceType = < SpaceType . OBSERVATION : 2 > , value : List = [], column_names : List = None ) \u2192 None","title":"method __init__"},{"location":"api/utils.spaces.discrete/#method-from_gym_space","text":"from_gym_space ( gym_space : Discrete , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > , column_names : List = None )","title":"method from_gym_space"},{"location":"api/utils.spaces.discrete/#method-to_gym_space","text":"to_gym_space () \u2192 Tuple [ Discrete , int ] This file was automatically generated via lazydocs .","title":"method to_gym_space"},{"location":"api/utils.spaces.factory/","text":"module utils.spaces.factory function space_value_from_gym space_value_from_gym ( gym_space : Space , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > ) \u2192 SpaceValue Create a SpaceValue instance from the given gym-space \"type\", the respective value and the type of the SpaceValue (action or observation). Args: gym_space : The space fo the gym environment (e.g., Discrete or Box) value : (observed) value (shape must fit the gym_space \"configuration\"), e.g. no value=2 for Discrete(2), since possible values would be 0, or 1. space_type : type the space is used for (this is, e.g., used for default column name generation) Returns SpaceValue for given gym_space and value. This file was automatically generated via lazydocs .","title":"Utils.spaces.factory"},{"location":"api/utils.spaces.factory/#module-utilsspacesfactory","text":"","title":"module utils.spaces.factory"},{"location":"api/utils.spaces.factory/#function-space_value_from_gym","text":"space_value_from_gym ( gym_space : Space , value , space_type : SpaceType = < SpaceType . OBSERVATION : 2 > ) \u2192 SpaceValue Create a SpaceValue instance from the given gym-space \"type\", the respective value and the type of the SpaceValue (action or observation). Args: gym_space : The space fo the gym environment (e.g., Discrete or Box) value : (observed) value (shape must fit the gym_space \"configuration\"), e.g. no value=2 for Discrete(2), since possible values would be 0, or 1. space_type : type the space is used for (this is, e.g., used for default column name generation) Returns SpaceValue for given gym_space and value. This file was automatically generated via lazydocs .","title":"function space_value_from_gym"},{"location":"api/utils.spaces/","text":"module utils.spaces This file was automatically generated via lazydocs .","title":"Utils.spaces"},{"location":"api/utils.spaces/#module-utilsspaces","text":"This file was automatically generated via lazydocs .","title":"module utils.spaces"},{"location":"api/utils.test_functions/","text":"module utils.test_functions function add_fake_noise add_fake_noise ( data , observation_columns , noise ) Returns the dimension of a given gym (action/ observation) space. This file was automatically generated via lazydocs .","title":"Utils.test functions"},{"location":"api/utils.test_functions/#module-utilstest_functions","text":"","title":"module utils.test_functions"},{"location":"api/utils.test_functions/#function-add_fake_noise","text":"add_fake_noise ( data , observation_columns , noise ) Returns the dimension of a given gym (action/ observation) space. This file was automatically generated via lazydocs .","title":"function add_fake_noise"},{"location":"api/wrapper.gym_step_handlers.acrobot/","text":"module wrapper.gym_step_handlers.acrobot class AcrobotStepHandler method get_done get_done ( step : int ) \u2192 bool method get_reward get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers.acrobot"},{"location":"api/wrapper.gym_step_handlers.acrobot/#module-wrappergym_step_handlersacrobot","text":"","title":"module wrapper.gym_step_handlers.acrobot"},{"location":"api/wrapper.gym_step_handlers.acrobot/#class-acrobotstephandler","text":"","title":"class AcrobotStepHandler"},{"location":"api/wrapper.gym_step_handlers.acrobot/#method-get_done","text":"get_done ( step : int ) \u2192 bool","title":"method get_done"},{"location":"api/wrapper.gym_step_handlers.acrobot/#method-get_reward","text":"get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"method get_reward"},{"location":"api/wrapper.gym_step_handlers.cartpole/","text":"module wrapper.gym_step_handlers.cartpole class CartPoleStepHandler method get_done get_done ( step : int ) \u2192 bool This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers.cartpole"},{"location":"api/wrapper.gym_step_handlers.cartpole/#module-wrappergym_step_handlerscartpole","text":"","title":"module wrapper.gym_step_handlers.cartpole"},{"location":"api/wrapper.gym_step_handlers.cartpole/#class-cartpolestephandler","text":"","title":"class CartPoleStepHandler"},{"location":"api/wrapper.gym_step_handlers.cartpole/#method-get_done","text":"get_done ( step : int ) \u2192 bool This file was automatically generated via lazydocs .","title":"method get_done"},{"location":"api/wrapper.gym_step_handlers.continuous_mountain_car/","text":"module wrapper.gym_step_handlers.continuous_mountain_car class ContinuousMountainCarStepHandler method get_done get_done ( step : int ) \u2192 bool method get_reward get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers.continuous mountain car"},{"location":"api/wrapper.gym_step_handlers.continuous_mountain_car/#module-wrappergym_step_handlerscontinuous_mountain_car","text":"","title":"module wrapper.gym_step_handlers.continuous_mountain_car"},{"location":"api/wrapper.gym_step_handlers.continuous_mountain_car/#class-continuousmountaincarstephandler","text":"","title":"class ContinuousMountainCarStepHandler"},{"location":"api/wrapper.gym_step_handlers.continuous_mountain_car/#method-get_done","text":"get_done ( step : int ) \u2192 bool","title":"method get_done"},{"location":"api/wrapper.gym_step_handlers.continuous_mountain_car/#method-get_reward","text":"get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"method get_reward"},{"location":"api/wrapper.gym_step_handlers/","text":"module wrapper.gym_step_handlers function get_step_handler_for_gym_env get_step_handler_for_gym_env ( gym_env_name : str , cfg : Configuration ) \u2192 StepRewardDoneHandler Return an example step handler for the given gym environemtn name, that uses the given config file. This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers"},{"location":"api/wrapper.gym_step_handlers/#module-wrappergym_step_handlers","text":"","title":"module wrapper.gym_step_handlers"},{"location":"api/wrapper.gym_step_handlers/#function-get_step_handler_for_gym_env","text":"get_step_handler_for_gym_env ( gym_env_name : str , cfg : Configuration ) \u2192 StepRewardDoneHandler Return an example step handler for the given gym environemtn name, that uses the given config file. This file was automatically generated via lazydocs .","title":"function get_step_handler_for_gym_env"},{"location":"api/wrapper.gym_step_handlers.mountain_car/","text":"module wrapper.gym_step_handlers.mountain_car class MountainCarStepHandler method get_done get_done ( step : int ) \u2192 bool method get_reward get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers.mountain car"},{"location":"api/wrapper.gym_step_handlers.mountain_car/#module-wrappergym_step_handlersmountain_car","text":"","title":"module wrapper.gym_step_handlers.mountain_car"},{"location":"api/wrapper.gym_step_handlers.mountain_car/#class-mountaincarstephandler","text":"","title":"class MountainCarStepHandler"},{"location":"api/wrapper.gym_step_handlers.mountain_car/#method-get_done","text":"get_done ( step : int ) \u2192 bool","title":"method get_done"},{"location":"api/wrapper.gym_step_handlers.mountain_car/#method-get_reward","text":"get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"method get_reward"},{"location":"api/wrapper.gym_step_handlers.pendulum/","text":"module wrapper.gym_step_handlers.pendulum function angle_normalize angle_normalize ( x ) class PendulumStepHandler method get_reward get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"Wrapper.gym step handlers.pendulum"},{"location":"api/wrapper.gym_step_handlers.pendulum/#module-wrappergym_step_handlerspendulum","text":"","title":"module wrapper.gym_step_handlers.pendulum"},{"location":"api/wrapper.gym_step_handlers.pendulum/#function-angle_normalize","text":"angle_normalize ( x )","title":"function angle_normalize"},{"location":"api/wrapper.gym_step_handlers.pendulum/#class-pendulumstephandler","text":"","title":"class PendulumStepHandler"},{"location":"api/wrapper.gym_step_handlers.pendulum/#method-get_reward","text":"get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"method get_reward"},{"location":"api/wrapper/","text":"module wrapper This file was automatically generated via lazydocs .","title":"Wrapper"},{"location":"api/wrapper/#module-wrapper","text":"This file was automatically generated via lazydocs .","title":"module wrapper"},{"location":"api/wrapper.reset_handler/","text":"module wrapper.reset_handler class ResetHandler A ResetHandler generates the \"reset-observation\" state for a (wrapped) gym environment. The default implementation calculates the reset observation based on the configuration. Custom handlers must overwrite the generate_reset_state() method. Derived classes/handlers also have access to the user configuration via the config class variable. method __init__ __init__ ( config : Configuration = None ) method generate_reset_observation generate_reset_observation ( action_space : Space , observation_space : Space , window_size : int = 1 ) \u2192 Tuple Generate and return a reset buffer and a current state for a gym environment. The returned buffer contains values for the given window_size minus 1, such that the wrapped gym environment can use the buffer plus the returned current state plus the first user (action) input to fill up to the required amount of items in the buffer. Params: action_space: The action space used by the gym environment. observation_space: The observation space used by the gym environment. window_size: The size of the buffer used by the model (neural network) of the wrapped gym environment. Returns tuple with buffer as first item and the current (observation) space as second item. This file was automatically generated via lazydocs .","title":"Wrapper.reset handler"},{"location":"api/wrapper.reset_handler/#module-wrapperreset_handler","text":"","title":"module wrapper.reset_handler"},{"location":"api/wrapper.reset_handler/#class-resethandler","text":"A ResetHandler generates the \"reset-observation\" state for a (wrapped) gym environment. The default implementation calculates the reset observation based on the configuration. Custom handlers must overwrite the generate_reset_state() method. Derived classes/handlers also have access to the user configuration via the config class variable.","title":"class ResetHandler"},{"location":"api/wrapper.reset_handler/#method-__init__","text":"__init__ ( config : Configuration = None )","title":"method __init__"},{"location":"api/wrapper.reset_handler/#method-generate_reset_observation","text":"generate_reset_observation ( action_space : Space , observation_space : Space , window_size : int = 1 ) \u2192 Tuple Generate and return a reset buffer and a current state for a gym environment. The returned buffer contains values for the given window_size minus 1, such that the wrapped gym environment can use the buffer plus the returned current state plus the first user (action) input to fill up to the required amount of items in the buffer. Params: action_space: The action space used by the gym environment. observation_space: The observation space used by the gym environment. window_size: The size of the buffer used by the model (neural network) of the wrapped gym environment. Returns tuple with buffer as first item and the current (observation) space as second item. This file was automatically generated via lazydocs .","title":"method generate_reset_observation"},{"location":"api/wrapper.step_handler/","text":"module wrapper.step_handler class StepRewardDoneHandler The reward handler is responsible for calculating the reward for the current state and the action that lead to the current state (get_reward method). Additionally, it must implement a get_done method that returns True unless the \"environment\" should stop. Within the class access to the current state and action as well as a history of both is given. The default implementation uses configuration parameters to calculate the reward and whether the current episode is finished, or not (thus, proper configuration parameters are required if using the default implementation). It is ok to only overwrite one of both functions (get_reward / get_done) in a custom implementation and make use of the default implementation for the other method. method __init__ __init__ ( config : Configuration = None ) method get_done get_done ( step : int ) \u2192 bool method get_reward get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"Wrapper.step handler"},{"location":"api/wrapper.step_handler/#module-wrapperstep_handler","text":"","title":"module wrapper.step_handler"},{"location":"api/wrapper.step_handler/#class-steprewarddonehandler","text":"The reward handler is responsible for calculating the reward for the current state and the action that lead to the current state (get_reward method). Additionally, it must implement a get_done method that returns True unless the \"environment\" should stop. Within the class access to the current state and action as well as a history of both is given. The default implementation uses configuration parameters to calculate the reward and whether the current episode is finished, or not (thus, proper configuration parameters are required if using the default implementation). It is ok to only overwrite one of both functions (get_reward / get_done) in a custom implementation and make use of the default implementation for the other method.","title":"class StepRewardDoneHandler"},{"location":"api/wrapper.step_handler/#method-__init__","text":"__init__ ( config : Configuration = None )","title":"method __init__"},{"location":"api/wrapper.step_handler/#method-get_done","text":"get_done ( step : int ) \u2192 bool","title":"method get_done"},{"location":"api/wrapper.step_handler/#method-get_reward","text":"get_reward ( step : int ) \u2192 float This file was automatically generated via lazydocs .","title":"method get_reward"},{"location":"api/wrapper.wrapped_model_env/","text":"module wrapper.wrapped_model_env class WrappedModelEnv This class wraps a dynamic model (created with the stable_baselines_model_based_rl library) as an Gym environment. This allows to train any policy on the environment using the dynmaic model in the background for predicting the next state/observation for any given action. method __init__ __init__ ( model_path , config : Configuration , step_handler : StepRewardDoneHandler = None , reset_handler : ResetHandler = None , window_size = None ) property unwrapped Completely unwrap this env. Returns: gym.Env : The base non-wrapped gym.Env instance method reset reset () method step step ( action ) This file was automatically generated via lazydocs .","title":"Wrapper.wrapped model env"},{"location":"api/wrapper.wrapped_model_env/#module-wrapperwrapped_model_env","text":"","title":"module wrapper.wrapped_model_env"},{"location":"api/wrapper.wrapped_model_env/#class-wrappedmodelenv","text":"This class wraps a dynamic model (created with the stable_baselines_model_based_rl library) as an Gym environment. This allows to train any policy on the environment using the dynmaic model in the background for predicting the next state/observation for any given action.","title":"class WrappedModelEnv"},{"location":"api/wrapper.wrapped_model_env/#method-__init__","text":"__init__ ( model_path , config : Configuration , step_handler : StepRewardDoneHandler = None , reset_handler : ResetHandler = None , window_size = None )","title":"method __init__"},{"location":"api/wrapper.wrapped_model_env/#property-unwrapped","text":"Completely unwrap this env. Returns: gym.Env : The base non-wrapped gym.Env instance","title":"property unwrapped"},{"location":"api/wrapper.wrapped_model_env/#method-reset","text":"reset ()","title":"method reset"},{"location":"api/wrapper.wrapped_model_env/#method-step","text":"step ( action ) This file was automatically generated via lazydocs .","title":"method step"}]}